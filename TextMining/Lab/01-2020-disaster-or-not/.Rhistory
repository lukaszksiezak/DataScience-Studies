n = 100; mi = 1; sigma = 2;
x = rnorm(n, mean = mi, sd = sigma)
# Korzystając z testu Kołmogorowa-Smirnowa zweryfikuj hipotezę, że dane pochodzą z rozkładu N(1,4) lub N(0,9):
ks.test(x, 'pnorm', mean = 1, sd = 2)
ks.test(x, 'pnorm', mean = 0, sd = 3)
# Za pomocą testu Shapiro-Wilka zweryfikuj hipotezę, że dane pochodzą z rozkładu normalnego
shapiro.test(x)
n = 100; a = 2; b = 4;
x = runif(n, min = a, max = b)
# Zweryfikuj hipotezę, że dane pochodzą z rozkładu normalnego korzystając z testu Shapiro-Wilka
shapiro.test(x)
# Powtórz weryfikację tej hipotezy korzystając z testu Kołmogorowa-Smirnowa:
ks.test(x, 'pnorm', mean = (a + b)/2, sd = sqrt((b - a)^2/12))
n = 100; a = 2; b = 4;
x = runif(n, min = a, max = b)
n = 100; a = 2; b = 4;
x = runif(n, min = a, max = b)
# Zweryfikuj hipotezę, że dane pochodzą z rozkładu normalnego korzystając z testu Shapiro-Wilka
shapiro.test(x)
n = 100; mi = 1; sigma = 2;
x = rnorm(n, mean = mi, sd = sigma)
# Za pomocą testu Shapiro-Wilka zweryfikuj hipotezę, że dane pochodzą z rozkładu normalnego
shapiro.test(x)
setwd("C:/Users/plluksi/source/coding/KaggleCompetitions/01-2020-disaster-or-not")
# Wczytuję zbiór treningowy i testowy
train_data <- read.csv("./data/train.csv", na.strings = c(""))
test_data <- read.csv("./data/test.csv", na.strings = c(""))
# Rzut okiem na 6 pierwszych wierszy
head(train_data)
train_data$id <- NULL
test_data$id <- NULL
missing_train_data <- colSums(sapply(train_data, is.na))
missing_test_data <- colSums(sapply(test_data, is.na))
# Jak to się ma procentowo do całości
missing_train_data_prop <- missing_train_data / nrow(train_data)
missing_test_data_prop <- missing_test_data / nrow(test_data)
train_data$location <- NULL
test_data$location <- NULL
dist_keyword<-unique(train_data$keyword)
dist_keyword
# Zobaczmy czy keyword w jakiś sposób będzie miał związek ze zmienną celu
library(dplyr)
train_data$keyword <- sapply(train_data$keyword, as.character)
train_data$keyword[is.na(train_data$keyword)] <- "no category"
by_target <- train_data %>% group_by(keyword) %>% summarize(target_mean = sum(target)/n()) %>% arrange(desc(target_mean))
View(by_target)
# Analiza dystrybucji tweetów względem zmiennej celu
library(ggplot2)
tweets_target_dist = train_data %>% group_by(target) %>% summarise(n())
barplot(tweets_target_dist$`n()`, main="Dystrybucja zmiennej celu w zbiorze treningowym",
names.arg=c("Disaster", "Not disaster"), ylim =c(0, 5000))
# Chmura słów przed procesowaniem
library(tm)
library(wordcloud)
library(corpus)
print_word_cloud <- function(tweetCorpus){
train_tdm <- TermDocumentMatrix(tweetCorpus[1:7613], control= list(wordLengths= c(1, Inf)))
(freq.terms <- findFreqTerms(train_tdm, lowfreq = 50))
train.term.freq <- rowSums(as.matrix(train_tdm))
train.term.freq <- subset(train.term.freq, train.term.freq > 50)
train_df <- data.frame(term = names(train.term.freq), freq= train.term.freq)
word.freq <-sort(rowSums(as.matrix((train_tdm))), decreasing= F)
pal<- brewer.pal(8, "Dark2")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal, max.words = 100)
}
tweet_corpus<- Corpus(VectorSource(train_data$text))
print_word_cloud(tweet_corpus)
library(tidytext)
library(stringi)
train_data %>% unnest_tokens(bigrams_in_tweets, text, token = "ngrams", n = 2) %>% count(bigrams_in_tweets, sort = TRUE) %>% slice(1:10) %>%
ggplot() + geom_bar(aes(bigrams_in_tweets, n), stat = "identity", fill = "#de5678") +
theme_minimal() +
coord_flip() +
labs(title = "Najwięcej bigramów w tweetach",
subtitle = "biblioteka: tidytext")
all_tweets_df <- bind_rows(train_data, test_data)
all_data_corpus <- Corpus(VectorSource(all_tweets_df$text))
writeLines(strwrap(all_data_corpus[[1325]]$content,60))
# treść tweetów do małych liter
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(
stri_trans_tolower))
# Ususnięcie linków (ulr-ów)
remove_urls <- function(x) {
gsub("http[^[:space:]]*", "", x)
}
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(
remove_urls))
# Usunięcie nazw użyttkowników
remove_usernames <- function(x){
gsub("@[^[:space:]]*", "", x)
}
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(
remove_usernames))
# usunięcie wszystkiego co nie jest tekstem
remove_anything_but_text <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(
remove_anything_but_text))
# stemming słów z textów tweetów
all_data_corpus <- tm_map(
all_data_corpus, stemDocument, "english")
# Usunięcie słów jedno i dwu wyrazowych
remove_one_char <- function(x){
gsub(" . ", " ", x)
}
remove_two_chars <- function(x){
gsub(" .. ", " ", x)
}
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(remove_one_char))
all_data_corpus <- tm_map(
all_data_corpus, content_transformer(remove_two_chars))
#install.packages('qdap')
library(qdap)
# Usunięcie angielskich stop-words
all_data_corpus <- tm_map(
all_data_corpus, removeWords, c(Top200Words,"amp", "via", "im"))
all_data_corpus<- tm_map(all_data_corpus, stripWhitespace)
writeLines(strwrap(all_data_corpus[[1325]]$content,60))
print_word_cloud(all_data_corpus)
train_data_corpus = all_data_corpus[0:7613]
test_data_corpus = all_data_corpus[7614:10876]
DTM <- DocumentTermMatrix(train_data_corpus)
inspect(DTM[1:10, 50:60])
sparse_DTM <- removeSparseTerms(DTM, 0.995)
tweetsSparse <- as.data.frame(as.matrix(sparse_DTM))
colnames(tweetsSparse) <- make.names(colnames(tweetsSparse))
tweetsSparse$target <- train_data$target
inspect(sparse_DTM[1:5, 50:60])
# nauka - podział na 70% danych treningowych i 30 testowych
smp_size <- floor(0.7 * nrow(tweetsSparse))
set.seed(123)
train_ind <- sample(seq_len(nrow(tweetsSparse)), size = smp_size)
train <- tweetsSparse[train_ind, ]
test <- tweetsSparse[-train_ind, ]
# ternowanie modelu - drzewo decyzyjne
library('rpart')
tweetCART <- rpart(target ~ . , data = train, method = "class")
predictCART <- predict(tweetCART, newdata = test, type = "class")
cmat_CART <- table(test$target, predictCART)
cmat_CART
accu_CART <- (cmat_CART[1,1] + cmat_CART[2,2])/sum(cmat_CART)
accu_CART
install.packages('RTextTools')
modelSVM <- train_model(train, "SVM", kernel="linear", cost=1)
model <- e1071::svm(news.source ~ .,
data = train, type = "nu-classification", kernal = "linear")
model <- e1071::svm(target ~ .,
data = train, type = "nu-classification", kernal = "linear")
predictions <- predict(model, test)
truth_table <- table(test$target, predictions)
truth_table
accu_svm <- (truth_table[1,1] + truth_table[2,2])/sum(truth_table)
accu_CART
e1071
model_svm <- e1071::svm(target ~ .,
data = train, type = "nu-classification", kernal = "gaussian")
predictions_svm <- predict(model_svm, test)
truth_table <- table(test$target, predictions_svm)
truth_table
accu_svm <- (truth_table[1,1] + truth_table[2,2])/sum(truth_table)
accu_CART
DTM_test <- DocumentTermMatrix(test_data_corpus)
sparse_DTM_test <- removeSparseTerms(DTM_test, 0.995)
tweetsSparse_test <- as.data.frame(as.matrix(sparse_DTM_test))
colnames(tweetsSparse_test) <- make.names(colnames(tweetsSparse_test))
predictions_competition <- predict(model_svm, tweetsSparse_test)
predictions_competition <- predict(model_svm, tweetsSparse_test)
tweetsSparse_test <- as.data.frame(as.matrix(sparse_DTM_test))
predictions_competition <- predict(model_svm, tweetsSparse_test)
accu_svm <- (truth_table[1,1] + truth_table[2,2])/sum(truth_table)
accu_svm
model_svm <- e1071::svm(target ~ .,
data = train, type = "nu-classification", kernal = "gaussian")
predictions_svm <- predict(model_svm, test)
truth_table <- table(test$target, predictions_svm)
truth_table
accu_svm <- (truth_table[1,1] + truth_table[2,2])/sum(truth_table)
accu_svm
tweets_target_dist = train_data %>% group_by(target) %>% summarise(n())
barplot(tweets_target_dist$`n()`, main="Dystrybucja zmiennej celu w zbiorze treningowym",
names.arg=c("Disaster", "Not disaster"), ylim =c(0, 5000))
train_data$keyword <- sapply(train_data$keyword, as.character)
train_data$keyword[is.na(train_data$keyword)] <- "no category"
by_target <- train_data %>% group_by(keyword) %>% summarize(target_mean = sum(target)/n()) %>% arrange(desc(target_mean))
View(by_target)
